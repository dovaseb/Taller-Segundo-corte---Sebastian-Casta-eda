{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83aab35bed5646d7b3cbefb6b9fdbfb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_72eb36993a524046937491f9b197552b"
          }
        },
        "88897308e8264253b81ab2149dcdedc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d6c5c15be754b2bb3af0990b9d8dc68",
            "placeholder": "​",
            "style": "IPY_MODEL_4dde2bcc74e8440796b3010e95a5c19d",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2e0aff56701b4ecfbf7a9eeb61f2eba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_19b85028e1844c23835827467181334a",
            "placeholder": "​",
            "style": "IPY_MODEL_d7eb2f751d8b401eaeb9568fc1d09b29",
            "value": ""
          }
        },
        "001602fce88847e19d9af2d2c347620e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_34b8a8f4834d4520a39696ac6f75b290",
            "style": "IPY_MODEL_c5a129bfa7654e3983a69962c464dc6e",
            "value": true
          }
        },
        "f2b6b8bee78143658d39236ba8b17e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8428a2ecf5cc46ca8af208f4064afb5f",
            "style": "IPY_MODEL_ea2c9544b1054eaaa7c5b3b44d2e34e5",
            "tooltip": ""
          }
        },
        "96e7cc02e08d4f47ae19338724ba3b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e99d3b0083d46f19beae7e09ac0c3d5",
            "placeholder": "​",
            "style": "IPY_MODEL_331d87b5ab1b44c8a936ddbc679a1f0b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "72eb36993a524046937491f9b197552b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4d6c5c15be754b2bb3af0990b9d8dc68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dde2bcc74e8440796b3010e95a5c19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19b85028e1844c23835827467181334a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7eb2f751d8b401eaeb9568fc1d09b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34b8a8f4834d4520a39696ac6f75b290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a129bfa7654e3983a69962c464dc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8428a2ecf5cc46ca8af208f4064afb5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2c9544b1054eaaa7c5b3b44d2e34e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1e99d3b0083d46f19beae7e09ac0c3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331d87b5ab1b44c8a936ddbc679a1f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d27ee50e4b5644e5a20e02cd9d41b3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce63735453ae45798bcbf3c36b0497ac",
            "placeholder": "​",
            "style": "IPY_MODEL_04b3952e79974bcdbeef8592e6aa7af3",
            "value": "Connecting..."
          }
        },
        "ce63735453ae45798bcbf3c36b0497ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b3952e79974bcdbeef8592e6aa7af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy4TGswGauoc",
        "outputId": "2aac2a9a-c897-4244-b724-12423bca7b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La caché de modelos se guardará en: ./cache_modelos\n",
            "No se encontró GPU disponible, utilizando CPU.\n",
            "Utilizando dispositivo: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 'gpt2' y tokenizador cargados exitosamente.\n",
            "Modelo cargado en la CPU.\n",
            "\n",
            "Prompt: 'Hola, ¿cómo estás?'\n",
            "Respuesta generada: 'Hola, ¿cómo estás? último?\n",
            "\n",
            "I'm sorry, but I don't know what to do. I'm not sure if I'll be able to get out of here, or if it's'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# TODO: Configurar las variables de entorno para la caché de modelos\n",
        "# Establecer la carpeta donde se almacenarán los modelos descargados\n",
        "ruta_cache = './cache_modelos'\n",
        "os.environ['TRANSFORMERS_CACHE'] = ruta_cache\n",
        "os.makedirs(ruta_cache, exist_ok=True)\n",
        "print(f\"La caché de modelos se guardará en: {ruta_cache}\")\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    \"\"\"\n",
        "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador)\n",
        "    \"\"\"\n",
        "    # TODO: Implementar la carga del modelo y tokenizador\n",
        "    # Utiliza AutoModelForCausalLM y AutoTokenizer\n",
        "    try:\n",
        "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "        print(f\"Modelo '{nombre_modelo}' y tokenizador cargados exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # TODO: Configurar el modelo para inferencia (evaluar y usar half-precision si es posible)\n",
        "    modelo.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        modelo = modelo.half().cuda()\n",
        "        print(\"Modelo cargado en la GPU y convertido a half-precision (float16).\")\n",
        "    else:\n",
        "        print(\"Modelo cargado en la CPU.\")\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    \"\"\"\n",
        "    Verifica el dispositivo disponible (CPU/GPU) y muestra información relevante.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Dispositivo a utilizar\n",
        "    \"\"\"\n",
        "    # TODO: Implementar la detección del dispositivo\n",
        "    if torch.cuda.is_available():\n",
        "        dispositivo = torch.device(\"cuda\")\n",
        "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA versión: {torch.version.cuda}\")\n",
        "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        dispositivo = torch.device(\"cpu\")\n",
        "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
        "\n",
        "    return dispositivo\n",
        "\n",
        "# Función principal de prueba\n",
        "def main():\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "\n",
        "    # TODO: Cargar un modelo pequeño adecuado para chatbots (ej. Mistral-7B, GPT2, etc.)\n",
        "    nombre_modelo = \"gpt2\"  # Un modelo pequeño y rápido para pruebas\n",
        "    modelo, tokenizador = cargar_modelo(nombre_modelo)\n",
        "\n",
        "    if modelo is not None and tokenizador is not None:\n",
        "        # TODO: Realizar una prueba simple de generación de texto\n",
        "        texto_prompt = \"Hola, ¿cómo estás?\"\n",
        "        input_ids = tokenizador.encode(texto_prompt, return_tensors=\"pt\").to(dispositivo)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = modelo.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, temperature=1.0)\n",
        "\n",
        "        texto_generado = tokenizador.decode(output[0], skip_special_tokens=True)\n",
        "        print(f\"\\nPrompt: '{texto_prompt}'\")\n",
        "        print(f\"Respuesta generada: '{texto_generado}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# (La configuración de la caché y la función cargar_modelo se mantienen del ejercicio anterior)\n",
        "ruta_cache = './cache_modelos'\n",
        "os.environ['TRANSFORMERS_CACHE'] = ruta_cache\n",
        "os.makedirs(ruta_cache, exist_ok=True)\n",
        "print(f\"La caché de modelos se guardará en: {ruta_cache}\")\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    try:\n",
        "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "        print(f\"Modelo '{nombre_modelo}' y tokenizador cargados exitosamente.\")\n",
        "\n",
        "        # Solución al error de padding para tokenizadores como GPT-2\n",
        "        if tokenizador.pad_token is None:\n",
        "            tokenizador.pad_token = tokenizador.eos_token\n",
        "            print(f\"Se estableció '{tokenizador.pad_token}' como pad_token.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    modelo.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        modelo = modelo.half().cuda()\n",
        "        print(\"Modelo cargado en la GPU y convertido a half-precision (float16).\")\n",
        "    else:\n",
        "        print(\"Modelo cargado en la CPU.\")\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512):\n",
        "    \"\"\"\n",
        "    Preprocesa el texto de entrada para pasarlo al modelo.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto de entrada del usuario\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        longitud_maxima (int): Longitud máxima de la secuencia\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor de entrada para el modelo\n",
        "    \"\"\"\n",
        "    # TODO: Implementar el preprocesamiento\n",
        "    # - Añadir tokens especiales si son necesarios (ej. [BOS], [SEP])\n",
        "    # - Convertir a tensor\n",
        "    # - Pasar al dispositivo correspondiente\n",
        "    entrada_tokenizada = tokenizador.encode_plus(\n",
        "        texto,\n",
        "        add_special_tokens=True,\n",
        "        max_length=longitud_maxima,\n",
        "        padding='longest',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    entrada_procesada = entrada_tokenizada['input_ids'].to(modelo.device)  # Usamos el dispositivo del modelo\n",
        "    return entrada_procesada\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    \"\"\"\n",
        "    Genera una respuesta basada en la entrada procesada.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo de lenguaje\n",
        "        entrada_procesada: Tokens de entrada procesados\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        parametros_generacion (dict): Parámetros para controlar la generación\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada\n",
        "    \"\"\"\n",
        "    # TODO: Implementar valores por defecto para parámetros de generación\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            'max_length': 100,\n",
        "            'num_beams': 5,\n",
        "            'no_repeat_ngram_size': 2,\n",
        "            'temperature': 1.0,\n",
        "            'top_p': 0.95\n",
        "        }\n",
        "\n",
        "    # TODO: Implementar la generación de texto\n",
        "    # Utilizar modelo.generate() con los parámetros adecuados\n",
        "    with torch.no_grad():\n",
        "        output = modelo.generate(\n",
        "            entrada_procesada,\n",
        "            **parametros_generacion\n",
        "        )\n",
        "\n",
        "    # TODO: Decodificar la salida y limpiar la respuesta\n",
        "    respuesta_tokenizada = output[:, entrada_procesada.shape[-1]:]  # Ignora el prompt en la salida\n",
        "    respuesta = tokenizador.decode(respuesta_tokenizada[0], skip_special_tokens=True)\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "def crear_prompt_sistema(instrucciones):\n",
        "    \"\"\"\n",
        "    Crea un prompt de sistema para dar instrucciones al modelo.\n",
        "\n",
        "    Args:\n",
        "        instrucciones (str): Instrucciones sobre cómo debe comportarse el chatbot\n",
        "\n",
        "    Returns:\n",
        "        str: Prompt formateado\n",
        "    \"\"\"\n",
        "    # TODO: Implementar la función para crear un prompt de sistema\n",
        "    # Muchos modelos modernos no requieren un formato especial para el prompt del sistema\n",
        "    # Se puede simplemente concatenar las instrucciones con la entrada del usuario.\n",
        "    # Sin embargo, para modelos específicos, podría haber formatos como:\n",
        "    # \"System: {instrucciones}\\n\\nUser: {entrada}\\n\\nAssistant:\"\n",
        "    # Por ahora, devolvemos las instrucciones directamente para mayor flexibilidad.\n",
        "    return instrucciones\n",
        "\n",
        "# Ejemplo de uso\n",
        "def interaccion_simple():\n",
        "    nombre_modelo = \"gpt2\"  # Puedes cambiarlo a otro modelo\n",
        "    modelo, tokenizador = cargar_modelo(nombre_modelo)\n",
        "\n",
        "    if modelo is None or tokenizador is None:\n",
        "        return\n",
        "\n",
        "    # TODO: Crear un prompt de sistema para definir la personalidad del chatbot\n",
        "    prompt_sistema = crear_prompt_sistema(\"Eres un chatbot amigable que responde preguntas de forma concisa.\")\n",
        "\n",
        "    while True:\n",
        "        entrada_usuario = input(\"Usuario: \")\n",
        "        if entrada_usuario.lower() == \"salir\":\n",
        "            break\n",
        "\n",
        "        # TODO: Procesar una entrada de ejemplo\n",
        "        prompt_completo = f\"{prompt_sistema} {entrada_usuario}\"\n",
        "        entrada_procesada = preprocesar_entrada(prompt_completo, tokenizador)\n",
        "\n",
        "        # TODO: Generar y mostrar la respuesta\n",
        "        respuesta = generar_respuesta(modelo, entrada_procesada, tokenizador)\n",
        "        print(f\"Chatbot: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interaccion_simple()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "3UFRbGogdSRb",
        "outputId": "5bdfc945-9e00-43e1-d5d3-b7460c9f4c30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La caché de modelos se guardará en: ./cache_modelos\n",
            "Modelo 'gpt2' y tokenizador cargados exitosamente.\n",
            "Se estableció '<|endoftext|>' como pad_token.\n",
            "Modelo cargado en la CPU.\n",
            "Usuario: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: , I'm here to answer your questions.\n",
            "\n",
            "Hello, my name is John and I am a software developer. I've been working on a project for a few years now and it's been a long time since I was able to get my hands on any kind of software. So I wanted to give you a little bit of background on what I do and how I got started.\n",
            "Usuario: how are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: \n",
            "\n",
            "I don't know how to answer that question, but I do know that I'm not the only one who doesn't want to talk about it. I've been doing this for a long time now, and I think it's time for me to step back and let the world know what I really think about this topic. It's important to me that we all\n",
            "Usuario: who are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot:  What is your name?\n",
            "\n",
            "You are a member of the community. You are currently logged in. Login | Sign up | Create a new account Username: Password: Don't have an account yet? Log in I agree to the Terms of Service and Privacy Policy . I also accept liability for any losses you may incur as a result of using this site.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-82256838ff63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0minteraccion_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-82256838ff63>\u001b[0m in \u001b[0;36minteraccion_simple\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mentrada_usuario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Usuario: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mentrada_usuario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# (La configuración de la caché y la función cargar_modelo se mantienen)\n",
        "ruta_cache = './cache_modelos'\n",
        "os.environ['TRANSFORMERS_CACHE'] = ruta_cache\n",
        "os.makedirs(ruta_cache, exist_ok=True)\n",
        "print(f\"La caché de modelos se guardará en: {ruta_cache}\")\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    try:\n",
        "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "        print(f\"Modelo '{nombre_modelo}' y tokenizador cargados exitosamente.\")\n",
        "        if tokenizador.pad_token is None:\n",
        "            tokenizador.pad_token = tokenizador.eos_token\n",
        "            print(f\"Se estableció '{tokenizador.pad_token}' como pad_token.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    modelo.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        modelo = modelo.half().cuda()\n",
        "        print(\"Modelo cargado en la GPU y convertido a half-precision (float16).\")\n",
        "    else:\n",
        "        print(\"Modelo cargado en la CPU.\")\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    if torch.cuda.is_available():\n",
        "        dispositivo = torch.device(\"cuda\")\n",
        "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA versión: {torch.version.cuda}\")\n",
        "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        dispositivo = torch.device(\"cpu\")\n",
        "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
        "    return dispositivo\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512):\n",
        "    entrada_tokenizada = tokenizador.encode_plus(\n",
        "        texto,\n",
        "        add_special_tokens=True,\n",
        "        max_length=longitud_maxima,\n",
        "        padding='longest',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    entrada_procesada = entrada_tokenizada['input_ids'].to(modelo.device)\n",
        "    return entrada_procesada\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    \"\"\"\n",
        "    Genera una respuesta basada en la entrada procesada.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo de lenguaje\n",
        "        entrada_procesada: Tokens de entrada procesados\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        parametros_generacion (dict): Parámetros para controlar la generación\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada\n",
        "    \"\"\"\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            'max_new_tokens': 100,  # Cambiado de 'max_length' a 'max_new_tokens'\n",
        "            'num_beams': 5,\n",
        "            'no_repeat_ngram_size': 2,\n",
        "            'temperature': 1.0,\n",
        "            'top_p': 0.95\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        output = modelo.generate(\n",
        "            entrada_procesada,\n",
        "            **parametros_generacion\n",
        "        )\n",
        "    respuesta_tokenizada = output[:, entrada_procesada.shape[-1]:]\n",
        "    respuesta = tokenizador.decode(respuesta_tokenizada[0], skip_special_tokens=True)\n",
        "    return respuesta\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        return f\"{rol.capitalize()}: {contenido}\"\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversación.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append({\"rol\": rol, \"contenido\": contenido})\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        mensajes_formateados = [self.formato_mensaje(msg[\"rol\"], msg[\"contenido\"]) for msg in self.historial]\n",
        "        return \"\\n\".join(mensajes_formateados)\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud máxima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        tokens_actuales = 0\n",
        "        historial_truncado = []\n",
        "\n",
        "        # Mantener el primer mensaje si es del sistema\n",
        "        if self.historial and self.historial[0][\"rol\"] == \"sistema\":\n",
        "            historial_truncado.append(self.historial[0])\n",
        "            tokens_actuales += len(tokenizador.encode(self.formato_mensaje(self.historial[0][\"rol\"], self.historial[0][\"contenido\"])))\n",
        "\n",
        "        # Iterar en orden inverso para mantener los mensajes más recientes\n",
        "        for mensaje in reversed(self.historial[1:] if self.historial and self.historial[0][\"rol\"] == \"sistema\" else self.historial):\n",
        "            mensaje_formateado = self.formato_mensaje(mensaje[\"rol\"], mensaje[\"contenido\"])\n",
        "            tokens_mensaje = len(tokenizador.encode(mensaje_formateado))\n",
        "            if tokens_actuales + tokens_mensaje <= self.longitud_maxima:\n",
        "                 # Insertar al principio si es el primer mensaje (después del sistema)\n",
        "                 # O insertar después del mensaje del sistema si ya hay uno\n",
        "                insert_index = 1 if self.historial and self.historial[0][\"rol\"] == \"sistema\" and len(historial_truncado) > 0 else 0\n",
        "                historial_truncado.insert(insert_index, mensaje)\n",
        "                tokens_actuales += tokens_mensaje\n",
        "            else:\n",
        "                break\n",
        "        self.historial = historial_truncado\n",
        "\n",
        "\n",
        "# Clase principal del chatbot\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementación de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo_id (str): Identificador del modelo en Hugging Face\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        self.modelo, self.tokenizador = cargar_modelo(modelo_id)\n",
        "        self.dispositivo = verificar_dispositivo()\n",
        "        # Ajustar la longitud máxima del contexto si es necesario, considerando el modelo utilizado.\n",
        "        # GPT-2 tiene una longitud de contexto máxima de 1024 tokens.\n",
        "        self.gestor_contexto = GestorContexto(longitud_maxima=self.tokenizador.model_max_length)\n",
        "\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Parámetros para la generación\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Construir el prompt completo\n",
        "        prompt_completo = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 3. Preprocesar la entrada (y truncar si es necesario antes de preprocesar)\n",
        "        # Truncamos el historial ANTES de construir el prompt y preprocesar para asegurar que\n",
        "        # el prompt_completo no exceda la longitud máxima manejable por el tokenizador/modelo.\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "        prompt_completo_truncado = self.gestor_contexto.construir_prompt_completo()\n",
        "        entrada_procesada = preprocesar_entrada(prompt_completo_truncado, self.tokenizador, longitud_maxima=self.tokenizador.model_max_length)\n",
        "\n",
        "\n",
        "        # 4. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada_procesada, self.tokenizador, parametros_generacion)\n",
        "\n",
        "        # 5. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        # 6. El truncado ya se realizó antes de generar la respuesta, si fue necesario.\n",
        "\n",
        "        # 7. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# Prueba del sistema\n",
        "def prueba_conversacion():\n",
        "    # Crear una instancia del chatbot\n",
        "    chatbot = Chatbot(\"gpt2\", instrucciones_sistema=\"Eres un asistente útil y conciso.\")\n",
        "\n",
        "    # Simular una conversación de varios turnos\n",
        "    print(\"Comienza la conversación (escribe 'salir' para terminar):\")\n",
        "    while True:\n",
        "        mensaje_usuario = input(\"Usuario: \")\n",
        "        if mensaje_usuario.lower() == \"salir\":\n",
        "            break\n",
        "        respuesta = chatbot.responder(mensaje_usuario)\n",
        "        print(f\"Asistente: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prueba_conversacion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pyHeh_lg17K",
        "outputId": "0e4c4c4e-9e12-4659-8873-7191bdb6b6f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La caché de modelos se guardará en: ./cache_modelos\n",
            "Modelo 'gpt2' y tokenizador cargados exitosamente.\n",
            "Se estableció '<|endoftext|>' como pad_token.\n",
            "Modelo cargado en la CPU.\n",
            "No se encontró GPU disponible, utilizando CPU.\n",
            "Comienza la conversación (escribe 'salir' para terminar):\n",
            "Usuario: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asistente: , my name is Usuari, and I'm here to talk to you about your project. I've been working on this project for a while now, so I thought I'd share it with you. This is my first project, but I wanted to give you a little bit more information about it. First of all, I want to say thank you to all the people who have helped me out with the project so far. It's been a long time since I had a project like this\n",
            "Usuario: how\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asistente:  did you get started with it?\n",
            "Eres: Well, it started when I was a student at the University of California at Berkeley, where I worked as a software engineer. The first thing I did was to write a program that would allow me to run a web application on a Raspberry Pi. That was the first time I really thought about how I could do something like that. Then I realized that I didn't have any programming experience at that time. So I decided to try it out.\n",
            "Usuario: salir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "83aab35bed5646d7b3cbefb6b9fdbfb9",
            "88897308e8264253b81ab2149dcdedc6",
            "2e0aff56701b4ecfbf7a9eeb61f2eba7",
            "001602fce88847e19d9af2d2c347620e",
            "f2b6b8bee78143658d39236ba8b17e36",
            "96e7cc02e08d4f47ae19338724ba3b53",
            "72eb36993a524046937491f9b197552b",
            "4d6c5c15be754b2bb3af0990b9d8dc68",
            "4dde2bcc74e8440796b3010e95a5c19d",
            "19b85028e1844c23835827467181334a",
            "d7eb2f751d8b401eaeb9568fc1d09b29",
            "34b8a8f4834d4520a39696ac6f75b290",
            "c5a129bfa7654e3983a69962c464dc6e",
            "8428a2ecf5cc46ca8af208f4064afb5f",
            "ea2c9544b1054eaaa7c5b3b44d2e34e5",
            "1e99d3b0083d46f19beae7e09ac0c3d5",
            "331d87b5ab1b44c8a936ddbc679a1f0b",
            "d27ee50e4b5644e5a20e02cd9d41b3d9",
            "ce63735453ae45798bcbf3c36b0497ac",
            "04b3952e79974bcdbeef8592e6aa7af3"
          ]
        },
        "id": "ML8sF7jZjHHp",
        "outputId": "29ccc186-ad43-41a8-fbe2-68527d5c60ac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83aab35bed5646d7b3cbefb6b9fdbfb9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACE_HUB_TOKEN'] = 'TU_TOKEN_AQUI' # Reemplaza con tu token real"
      ],
      "metadata": {
        "id": "xHBAWO-kjb_K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyxnFKVSovFM",
        "outputId": "e3645b4d-f9fd-459c-f0ba-3dabfa83227b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "a2ffe6588cb04ac5b142df6cc74168d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# --- Funciones de los ejercicios anteriores (para cargar el modelo) ---\n",
        "ruta_cache = './cache_modelos'\n",
        "os.environ['TRANSFORMERS_CACHE'] = ruta_cache\n",
        "os.makedirs(ruta_cache, exist_ok=True)\n",
        "print(f\"La caché de modelos se guardará en: {ruta_cache}\")\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    if torch.cuda.is_available():\n",
        "        dispositivo = torch.device(\"cuda\")\n",
        "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA versión: {torch.version.cuda}\")\n",
        "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        dispositivo = torch.device(\"cpu\")\n",
        "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
        "    return dispositivo\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    try:\n",
        "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "        print(f\"Modelo '{nombre_modelo}' y tokenizador cargados exitosamente.\")\n",
        "        if tokenizador.pad_token is None:\n",
        "            tokenizador.pad_token = tokenizador.eos_token\n",
        "            print(f\"Se estableció '{tokenizador.pad_token}' como pad_token.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    modelo.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        modelo = modelo.half().cuda()\n",
        "        print(\"Modelo cargado en la GPU y convertido a half-precision (float16).\")\n",
        "    else:\n",
        "        print(\"Modelo cargado en la CPU.\")\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "# --- Funciones para la optimización (Ejercicio 4) ---\n",
        "def configurar_cuantizacion(bits=4, double_quant=True, llm_int8_threshold=6.0, quant_type='nf4'):\n",
        "    \"\"\"\n",
        "    Configura los parámetros para la cuantización del modelo.\n",
        "\n",
        "    Args:\n",
        "        bits (int): Bits para cuantización (4 u 8)\n",
        "        double_quant (bool): Usar doble cuantización para mayor ahorro de memoria (solo para 4 bits)\n",
        "        llm_int8_threshold (float): Umbral para la cuantización int8 (para modelos que lo soportan)\n",
        "        quant_type (str): Tipo de cuantización ('nf4' o 'bnb4' para 4 bits, 'int8' para 8 bits)\n",
        "\n",
        "    Returns:\n",
        "        BitsAndBytesConfig: Configuración de cuantización\n",
        "    \"\"\"\n",
        "    if bits not in [4, 8]:\n",
        "        raise ValueError(\"Los bits deben ser 4 u 8.\")\n",
        "\n",
        "    if bits == 4:\n",
        "        config_cuantizacion = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=double_quant,\n",
        "            bnb_4bit_quant_type=quant_type,\n",
        "            bnb_4bit_compute_dtype=torch.float16  # Recomendado para rendimiento\n",
        "        )\n",
        "    elif bits == 8:\n",
        "        config_cuantizacion = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            llm_int8_threshold=llm_int8_threshold\n",
        "        )\n",
        "    return config_cuantizacion\n",
        "\n",
        "def cargar_modelo_optimizado(nombre_modelo, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo\n",
        "        optimizaciones (dict): Diccionario con flags para las optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador)\n",
        "    \"\"\"\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    config = AutoConfig.from_pretrained(nombre_modelo)\n",
        "    load_in_8bit = False\n",
        "    load_in_4bit = False\n",
        "    quantization_config = None\n",
        "\n",
        "    if optimizaciones is None:\n",
        "        optimizaciones = {\n",
        "            \"cuantizacion\": False,\n",
        "            \"bits\": 4,\n",
        "            \"double_quant\": True,\n",
        "            \"llm_int8_threshold\": 6.0,\n",
        "            \"quant_type\": 'nf4',\n",
        "            \"offload_cpu\": False,\n",
        "            \"flash_attention\": False, # Flash Attention 2 se habilita directamente en la config si es compatible\n",
        "            \"sliding_window\": None # Tamaño de la ventana deslizante\n",
        "        }\n",
        "\n",
        "    if optimizaciones.get(\"cuantizacion\", False):\n",
        "        if optimizaciones.get(\"bits\") == 4:\n",
        "            load_in_4bit = True\n",
        "            quantization_config = configurar_cuantizacion(\n",
        "                bits=4,\n",
        "                double_quant=optimizaciones.get(\"double_quant\", True),\n",
        "                quant_type=optimizaciones.get(\"quant_type\", 'nf4')\n",
        "            )\n",
        "        elif optimizaciones.get(\"bits\") == 8:\n",
        "            load_in_8bit = True\n",
        "            quantization_config = configurar_cuantizacion(\n",
        "                bits=8,\n",
        "                llm_int8_threshold=optimizaciones.get(\"llm_int8_threshold\", 6.0)\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(\n",
        "            nombre_modelo,\n",
        "            load_in_8bit=load_in_8bit,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() and (load_in_4bit or load_in_8bit) else None,\n",
        "            device_map=\"auto\" if optimizaciones.get(\"offload_cpu\") else None,\n",
        "            attn_implementation=\"flash_attention_2\" if optimizaciones.get(\"flash_attention\") and config.attn_implementation == \"flash_attention_2\" else \"eager\"\n",
        "        )\n",
        "        print(f\"Modelo '{nombre_modelo}' cargado con optimizaciones: {optimizaciones}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}' con optimizaciones: {e}\")\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo, torch_dtype=torch.float16 if torch.cuda.is_available() else None, device_map=\"auto\")\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "def aplicar_sliding_window(modelo, window_size=1024):\n",
        "    \"\"\"\n",
        "    Configura la atención de ventana deslizante para procesar secuencias largas.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a configurar\n",
        "        window_size (int): Tamaño de la ventana de atención\n",
        "    \"\"\"\n",
        "    config = modelo.config\n",
        "    if hasattr(config, \"attn_config\") and hasattr(config.attn_config, \"sliding_window\"):\n",
        "        config.attn_config.sliding_window = window_size\n",
        "        modelo.config = config\n",
        "        print(f\"Atención de ventana deslizante configurada con tamaño: {window_size}\")\n",
        "    elif hasattr(config, \"sliding_window\"):\n",
        "        config.sliding_window = window_size\n",
        "        modelo.config = config\n",
        "        print(f\"Atención de ventana deslizante configurada con tamaño: {window_size}\")\n",
        "    else:\n",
        "        print(f\"El modelo {modelo.__class__.__name__} no soporta directamente la atención de ventana deslizante.\")\n",
        "\n",
        "def evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_runs=5):\n",
        "    \"\"\"\n",
        "    Evalúa el rendimiento del modelo en términos de velocidad y memoria.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a evaluar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        texto_prueba (str): Texto para pruebas de rendimiento\n",
        "        dispositivo: Dispositivo donde se ejecutará\n",
        "        num_runs (int): Número de veces para ejecutar la inferencia y calcular el promedio\n",
        "\n",
        "    Returns:\n",
        "        dict: Métricas de rendimiento\n",
        "    \"\"\"\n",
        "    modelo.to(dispositivo)\n",
        "    modelo.eval()\n",
        "\n",
        "    input_ids = tokenizador.encode(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
        "    input_length = input_ids.shape[-1]\n",
        "\n",
        "    times = []\n",
        "    memory_usage = []\n",
        "    for _ in range(num_runs):\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            _ = modelo.generate(input_ids, max_length=input_length + 50)\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "        memory_usage.append(torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0)\n",
        "\n",
        "    avg_time = sum(times) / num_runs\n",
        "    avg_memory = sum(memory_usage) / num_runs\n",
        "    tokens_per_second = (input_length * num_runs) / sum(times) if sum(times) > 0 else 0\n",
        "\n",
        "    metricas = {\n",
        "        \"inference_time_avg (seconds)\": f\"{avg_time:.4f}\",\n",
        "        \"memory_usage_max_avg (MB)\": f\"{avg_memory:.2f}\",\n",
        "        \"tokens_per_second\": f\"{tokens_per_second:.2f}\",\n",
        "        \"device\": dispositivo\n",
        "    }\n",
        "    return metricas\n",
        "\n",
        "# Función de demostración\n",
        "def demo_optimizaciones(nombre_modelo=\"gpt2\", texto_prueba=\"La inteligencia artificial es\"):\n",
        "    \"\"\"\n",
        "    Crea y evalúa diferentes configuraciones del modelo para comparar el rendimiento.\n",
        "    \"\"\"\n",
        "    print(f\"Evaluando el modelo: {nombre_modelo}\")\n",
        "    dispositivo = verificar_dispositivo()\n",
        "\n",
        "    print(\"\\n--- Modelo Base (float16) ---\")\n",
        "    modelo_base, tokenizador_base = cargar_modelo(nombre_modelo)\n",
        "    if modelo_base:\n",
        "        metrics_base = evaluar_rendimiento(modelo_base, tokenizador_base, texto_prueba, dispositivo)\n",
        "        print(metrics_base)\n",
        "        del modelo_base\n",
        "        del tokenizador_base\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"\\n--- Modelo con Cuantización 4 bits (nf4, double_quant=True) ---\")\n",
        "    modelo_4bit, tokenizador_4bit = cargar_modelo_optimizado(nombre_modelo, {\"cuantizacion\": True, \"bits\": 4, \"double_quant\": True, \"quant_type\": 'nf4'})\n",
        "    if modelo_4bit:\n",
        "        metrics_4bit = evaluar_rendimiento(modelo_4bit, tokenizador_4bit, texto_prueba, dispositivo)\n",
        "        print(metrics_4bit)\n",
        "        del modelo_4bit\n",
        "        del tokenizador_4bit\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Verificar si el modelo soporta sliding window attention\n",
        "    config = AutoConfig.from_pretrained(nombre_modelo)\n",
        "    supports_sliding_window = hasattr(config, \"attn_config\") and hasattr(config.attn_config, \"sliding_window\") or hasattr(config, \"sliding_window\")\n",
        "\n",
        "    if supports_sliding_window:\n",
        "        print(\"\\n--- Modelo con Sliding Window Attention (window_size=512) ---\")\n",
        "        modelo_sw, tokenizador_sw = cargar_modelo(nombre_modelo)\n",
        "        if modelo_sw:\n",
        "            aplicar_sliding_window(modelo_sw, window_size=512)\n",
        "            metrics_sw = evaluar_rendimiento(modelo_sw, tokenizador_sw, texto_prueba, dispositivo)\n",
        "            print(metrics_sw)\n",
        "            del modelo_sw\n",
        "            del tokenizador_sw\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    else:\n",
        "        print(f\"\\nEl modelo {nombre_modelo} no soporta directamente Sliding Window Attention.\")\n",
        "\n",
        "    print(\"\\n--- Modelo con Cuantización 4 bits y (si es compatible) Flash Attention ---\")\n",
        "    optims_all = {\"cuantizacion\": True, \"bits\": 4, \"double_quant\": True, \"quant_type\": 'nf4', \"flash_attention\": True}\n",
        "    modelo_all, tokenizador_all = cargar_modelo_optimizado(nombre_modelo, optims_all)\n",
        "    if modelo_all:\n",
        "        metrics_all = evaluar_rendimiento(modelo_all, tokenizador_all, texto_prueba, dispositivo)\n",
        "        print(metrics_all)\n",
        "        del modelo_all\n",
        "        del tokenizador_all\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_optimizaciones()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L8j0LwsiWfj",
        "outputId": "3bc82a2c-d187-4275-b56d-ffae14cd6333"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La caché de modelos se guardará en: ./cache_modelos\n",
            "Evaluando el modelo: gpt2\n",
            "No se encontró GPU disponible, utilizando CPU.\n",
            "\n",
            "--- Modelo Base (float16) ---\n",
            "Modelo 'gpt2' y tokenizador cargados exitosamente.\n",
            "Se estableció '<|endoftext|>' como pad_token.\n",
            "Modelo cargado en la CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inference_time_avg (seconds)': '3.6605', 'memory_usage_max_avg (MB)': '0.00', 'tokens_per_second': '1.64', 'device': device(type='cpu')}\n",
            "\n",
            "--- Modelo con Cuantización 4 bits (nf4, double_quant=True) ---\n",
            "Error al cargar el modelo 'gpt2' con optimizaciones: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inference_time_avg (seconds)': '3.7219', 'memory_usage_max_avg (MB)': '0.00', 'tokens_per_second': '1.61', 'device': device(type='cpu')}\n",
            "\n",
            "El modelo gpt2 no soporta directamente Sliding Window Attention.\n",
            "\n",
            "--- Modelo con Cuantización 4 bits y (si es compatible) Flash Attention ---\n",
            "Error al cargar el modelo 'gpt2' con optimizaciones: 'GPT2Config' object has no attribute 'attn_implementation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inference_time_avg (seconds)': '3.6172', 'memory_usage_max_avg (MB)': '0.00', 'tokens_per_second': '1.66', 'device': device(type='cpu')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio peft transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lucIQe-zqWHU",
        "outputId": "e76184ee-abbf-4ead-f8a4-16df21ba194c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
        "import os\n",
        "import gradio as gr\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# --- Configuración de Caché y Dispositivo (de ejercicios anteriores) ---\n",
        "ruta_cache = './cache_modelos'\n",
        "os.environ['TRANSFORMERS_CACHE'] = ruta_cache\n",
        "os.makedirs(ruta_cache, exist_ok=True)\n",
        "print(f\"La caché de modelos se guardará en: {ruta_cache}\")\n",
        "\n",
        "# Variable global para el modelo y tokenizador (simplifica el acceso en Gradio)\n",
        "# Se inicializarán en main_despliegue o al cargar un modelo personalizado\n",
        "modelo_global = None\n",
        "tokenizador_global = None\n",
        "dispositivo_global = None\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    \"\"\"\n",
        "    Verifica el dispositivo disponible (CPU/GPU) y muestra información relevante.\n",
        "    \"\"\"\n",
        "    global dispositivo_global\n",
        "    if torch.cuda.is_available():\n",
        "        dispositivo_global = torch.device(\"cuda\")\n",
        "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA versión: {torch.version.cuda}\")\n",
        "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        dispositivo_global = torch.device(\"cpu\")\n",
        "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
        "    return dispositivo_global\n",
        "\n",
        "def cargar_modelo_base(nombre_modelo, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo base, opcionalmente con optimizaciones como cuantización.\n",
        "    Similar a cargar_modelo_optimizado del Codigo 4.\n",
        "    \"\"\"\n",
        "    global modelo_global, tokenizador_global, dispositivo_global\n",
        "    if dispositivo_global is None:\n",
        "        dispositivo_global = verificar_dispositivo()\n",
        "\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    config = AutoConfig.from_pretrained(nombre_modelo)\n",
        "\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        print(f\"Se estableció '{tokenizador.pad_token}' como pad_token para el tokenizador.\")\n",
        "\n",
        "    load_in_8bit = False\n",
        "    load_in_4bit = False\n",
        "    quantization_config_bnb = None # Renombrado para evitar conflicto\n",
        "\n",
        "    if optimizaciones is None:\n",
        "        optimizaciones = {\n",
        "            \"cuantizacion\": False, # Por defecto no cuantizar al cargar modelo base para fine-tuning\n",
        "        }\n",
        "\n",
        "    if optimizaciones.get(\"cuantizacion\", False) and dispositivo_global.type == 'cuda':\n",
        "        bits = optimizaciones.get(\"bits\", 4) # Default a 4 bits si cuantización está activa\n",
        "        if bits == 4:\n",
        "            load_in_4bit = True\n",
        "            quantization_config_bnb = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=optimizaciones.get(\"double_quant\", True),\n",
        "                bnb_4bit_quant_type=optimizaciones.get(\"quant_type\", 'nf4'),\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "            print(\"Configurando cuantización de 4 bits.\")\n",
        "        elif bits == 8:\n",
        "            load_in_8bit = True\n",
        "            quantization_config_bnb = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=optimizaciones.get(\"llm_int8_threshold\", 6.0)\n",
        "            )\n",
        "            print(\"Configurando cuantización de 8 bits.\")\n",
        "    else:\n",
        "        print(\"Cuantización no habilitada o no hay GPU disponible. Cargando en float16/float32.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(\n",
        "            nombre_modelo,\n",
        "            quantization_config=quantization_config_bnb if (load_in_4bit or load_in_8bit) else None,\n",
        "            torch_dtype=torch.float16 if dispositivo_global.type == 'cuda' and not (load_in_4bit or load_in_8bit) else (torch.bfloat16 if load_in_4bit or load_in_8bit else None), # bfloat16 para cómputo con cuantización\n",
        "            device_map=\"auto\" if dispositivo_global.type == 'cuda' else None, # device_map=\"auto\" para múltiples GPUs o offloading\n",
        "            attn_implementation=\"flash_attention_2\" if optimizaciones.get(\"flash_attention\", False) and hasattr(config, \"attn_implementation\") and config.attn_implementation == \"flash_attention_2\" and dispositivo_global.type == 'cuda' else \"eager\",\n",
        "        )\n",
        "        print(f\"Modelo '{nombre_modelo}' cargado exitosamente.\")\n",
        "        if not (load_in_4bit or load_in_8bit) and dispositivo_global.type == 'cpu':\n",
        "             print(\"Modelo cargado en CPU en precisión completa (float32).\")\n",
        "        elif not (load_in_4bit or load_in_8bit) and dispositivo_global.type == 'cuda':\n",
        "            print(\"Modelo cargado en GPU en half-precision (float16).\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo '{nombre_modelo}' con optimizaciones: {e}\")\n",
        "        print(\"Intentando cargar el modelo sin optimizaciones específicas de cuantización/dtype en CPU.\")\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo) # Fallback más simple\n",
        "        if dispositivo_global.type == 'cuda':\n",
        "            modelo = modelo.to(dispositivo_global)\n",
        "\n",
        "\n",
        "    modelo_global = modelo\n",
        "    tokenizador_global = tokenizador\n",
        "    modelo_global.eval() # Por defecto en modo evaluación\n",
        "    return modelo_global, tokenizador_global\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512):\n",
        "    global modelo_global # Necesitamos acceso al dispositivo del modelo\n",
        "    if modelo_global is None:\n",
        "        raise ValueError(\"El modelo global no ha sido cargado.\")\n",
        "    if tokenizador is None:\n",
        "        raise ValueError(\"El tokenizador global no ha sido cargado.\")\n",
        "\n",
        "    entrada_tokenizada = tokenizador.encode_plus(\n",
        "        texto,\n",
        "        add_special_tokens=True,\n",
        "        max_length=longitud_maxima,\n",
        "        padding='longest',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    # Asegúrate de que el dispositivo del modelo_global sea el correcto\n",
        "    entrada_procesada = entrada_tokenizada['input_ids'].to(modelo_global.device)\n",
        "    return entrada_procesada\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            'max_new_tokens': 100,\n",
        "            'num_beams': 3, # Reducido para posible ejecución en CPU\n",
        "            'no_repeat_ngram_size': 2,\n",
        "            'temperature': 0.8, # Ligeramente menos aleatorio\n",
        "            'top_p': 0.92,\n",
        "            'pad_token_id': tokenizador.eos_token_id # Importante para generación correcta\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        output = modelo.generate(\n",
        "            entrada_procesada,\n",
        "            **parametros_generacion\n",
        "        )\n",
        "    # Decodificar solo los tokens nuevos generados\n",
        "    respuesta_tokenizada = output[:, entrada_procesada.shape[-1]:]\n",
        "    respuesta = tokenizador.decode(respuesta_tokenizada[0], skip_special_tokens=True)\n",
        "    return respuesta.strip()\n",
        "\n",
        "class GestorContexto:\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima # En tokens\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        return f\"{rol.capitalize()}: {contenido}\"\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        self.historial.append({\"rol\": rol, \"contenido\": contenido})\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        mensajes_formateados = [self.formato_mensaje(msg[\"rol\"], msg[\"contenido\"]) for msg in self.historial]\n",
        "        return \"\\n\".join(mensajes_formateados)\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        global tokenizador_global\n",
        "        if tokenizador is None: # Usa el global si no se pasa uno específico\n",
        "            tokenizador = tokenizador_global\n",
        "        if tokenizador is None:\n",
        "            print(\"Advertencia: No hay tokenizador para truncar historial.\")\n",
        "            return\n",
        "\n",
        "        tokens_totales_aproximados = 0\n",
        "        # Primero calcular tokens de todo el historial formateado\n",
        "        prompt_actual = self.construir_prompt_completo()\n",
        "        tokens_actuales = len(tokenizador.encode(prompt_actual))\n",
        "\n",
        "        while tokens_actuales > self.longitud_maxima and len(self.historial) > 1:\n",
        "            # Priorizar eliminar mensajes antiguos, excepto el de sistema si existe\n",
        "            if self.historial[0][\"rol\"] == \"sistema\" and len(self.historial) > 1:\n",
        "                del self.historial[1] # Elimina el mensaje más antiguo después del de sistema\n",
        "            else:\n",
        "                del self.historial[0] # Elimina el mensaje más antiguo\n",
        "\n",
        "            prompt_actual = self.construir_prompt_completo() # Reconstruir para recalcular\n",
        "            tokens_actuales = len(tokenizador.encode(prompt_actual))\n",
        "\n",
        "        if tokens_actuales > self.longitud_maxima and len(self.historial) == 1:\n",
        "             # Si incluso un solo mensaje (o el de sistema) es demasiado largo, truncarlo\n",
        "            print(\"Advertencia: El mensaje restante es demasiado largo, se truncará su contenido.\")\n",
        "            mensaje_unico = self.historial[0]\n",
        "            ids_truncados = tokenizador.encode(mensaje_unico[\"contenido\"], max_length=self.longitud_maxima, truncation=True)\n",
        "            self.historial[0][\"contenido\"] = tokenizador.decode(ids_truncados, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, modelo_id_o_path, instrucciones_sistema=None, es_personalizado=False, optimizaciones_carga=None):\n",
        "        global modelo_global, tokenizador_global, dispositivo_global\n",
        "        self.dispositivo = verificar_dispositivo() # Asegura que dispositivo_global esté seteado\n",
        "\n",
        "        if es_personalizado:\n",
        "            print(f\"Cargando modelo PEFT personalizado desde: {modelo_id_o_path}\")\n",
        "            self.modelo, self.tokenizador = cargar_modelo_personalizado(modelo_id_o_path)\n",
        "        else:\n",
        "            print(f\"Cargando modelo base: {modelo_id_o_path}\")\n",
        "            self.modelo, self.tokenizador = cargar_modelo_base(modelo_id_o_path, optimizaciones=optimizaciones_carga)\n",
        "\n",
        "        modelo_global = self.modelo # Actualiza globales\n",
        "        tokenizador_global = self.tokenizador\n",
        "\n",
        "        # Determinar longitud máxima del contexto del modelo si está disponible\n",
        "        model_max_len = getattr(self.tokenizador, 'model_max_length', 1024)\n",
        "        if hasattr(self.modelo, 'config') and hasattr(self.modelo.config, 'max_position_embeddings'):\n",
        "            model_max_len = self.modelo.config.max_position_embeddings\n",
        "\n",
        "        self.gestor_contexto = GestorContexto(longitud_maxima=model_max_len // 2) # Usar la mitad para dar espacio a la respuesta\n",
        "\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        if self.modelo is None or self.tokenizador is None:\n",
        "            return \"Error: El modelo o el tokenizador no están cargados.\"\n",
        "\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador) # Truncar antes de construir prompt\n",
        "\n",
        "        prompt_completo = self.gestor_contexto.construir_prompt_completo()\n",
        "        # Añadir un sufijo que indique al modelo que debe generar la respuesta del asistente\n",
        "        # Esto es crucial para muchos modelos instructivos o de chat.\n",
        "        if not prompt_completo.endswith(\"Asistente:\"): # Evitar duplicados si ya está\n",
        "             prompt_completo += \"\\nAsistente:\"\n",
        "\n",
        "\n",
        "        # Usar model_max_length del tokenizador para preprocesar_entrada\n",
        "        max_len_tokenizador = getattr(self.tokenizador, 'model_max_length', 1024)\n",
        "\n",
        "        entrada_procesada = preprocesar_entrada(prompt_completo, self.tokenizador, longitud_maxima=max_len_tokenizador)\n",
        "\n",
        "        respuesta_chatbot = generar_respuesta(self.modelo, entrada_procesada, self.tokenizador, parametros_generacion)\n",
        "\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta_chatbot)\n",
        "        # No es necesario truncar de nuevo aquí si se hizo bien antes.\n",
        "\n",
        "        return respuesta_chatbot\n",
        "\n",
        "# --- Funciones del Ejercicio 5 ---\n",
        "\n",
        "def configurar_peft(modelo, r=8, lora_alpha=32, lora_dropout=0.05, task_type=TaskType.CAUSAL_LM, target_modules=None):\n",
        "    \"\"\"\n",
        "    Configura el modelo para fine-tuning con PEFT/LoRA.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo base a adaptar.\n",
        "        r (int): Rango de adaptadores LoRA (dimensión de las matrices de bajo rango).\n",
        "        lora_alpha (int): Escala alpha para LoRA. Es un hiperparámetro que escala los pesos aprendidos.\n",
        "                         Una práctica común es poner lora_alpha = 2 * r.\n",
        "        lora_dropout (float): Dropout para las capas LoRA.\n",
        "        task_type: Tipo de tarea para PEFT (ej. CAUSAL_LM).\n",
        "        target_modules (list of str, opcional): Nombres de los módulos a los que aplicar LoRA.\n",
        "                            Si es None, PEFT intentará inferirlos (puede no ser óptimo para todos los modelos).\n",
        "                            Ejemplos: [\"q_proj\", \"v_proj\"] para muchos transformadores.\n",
        "                            Para GPT2: [\"c_attn\"] o ser más específico como [\"c_attn.q_proj\", \"c_attn.v_proj\"] si c_attn es un nn.Linear grande.\n",
        "                            Para modelos como Llama: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "    Returns:\n",
        "        modelo_peft: Modelo adaptado para fine-tuning.\n",
        "    \"\"\"\n",
        "    # TODO: Implementar la configuración de PEFT\n",
        "    if target_modules is None:\n",
        "        # Intento genérico para modelos tipo GPT-2/GPT-J/NeoX\n",
        "        print(\"Advertencia: target_modules no especificado. PEFT intentará inferir.\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    config_lora = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=target_modules, # Dejar que PEFT infiera si es None y el modelo lo soporta\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",  # 'none', 'all', or 'lora_only'. 'none' es común.\n",
        "        task_type=task_type\n",
        "    )\n",
        "\n",
        "\n",
        "    # Si el modelo ya está cuantizado (ej. load_in_4bit/8bit), PEFT necesita preparación especial\n",
        "    if getattr(modelo, \"is_loaded_in_8bit\", False) or getattr(modelo, \"is_loaded_in_4bit\", False):\n",
        "        from peft import prepare_model_for_kbit_training\n",
        "        print(\"Preparando modelo cuantizado para entrenamiento con PEFT...\")\n",
        "        modelo = prepare_model_for_kbit_training(\n",
        "            modelo, use_gradient_checkpointing=True # GC es muy recomendado aquí\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        modelo_peft = get_peft_model(modelo, config_lora)\n",
        "        print(\"Modelo configurado con PEFT/LoRA.\")\n",
        "        modelo_peft.print_trainable_parameters()\n",
        "    except Exception as e:\n",
        "        print(f\"Error al aplicar get_peft_model: {e}\")\n",
        "        print(\"Asegúrate de que 'target_modules' sea compatible con la arquitectura del modelo si PEFT no puede inferirlos.\")\n",
        "        return modelo # Devuelve el modelo original si falla\n",
        "\n",
        "    return modelo_peft\n",
        "\n",
        "def guardar_modelo_peft(modelo_peft, tokenizador, ruta_base, nombre_adaptador=\"lora_chatbot\"):\n",
        "    \"\"\"\n",
        "    Guarda los adaptadores PEFT (LoRA) y el tokenizador.\n",
        "    El modelo base no se guarda aquí, solo los adaptadores.\n",
        "\n",
        "    Args:\n",
        "        modelo_peft: Modelo con adaptadores PEFT.\n",
        "        tokenizador: Tokenizador del modelo.\n",
        "        ruta_base (str): Directorio base donde se creará una subcarpeta para el adaptador.\n",
        "        nombre_adaptador (str): Nombre de la subcarpeta para el adaptador.\n",
        "    \"\"\"\n",
        "    ruta_adaptador = os.path.join(ruta_base, nombre_adaptador)\n",
        "    os.makedirs(ruta_adaptador, exist_ok=True)\n",
        "    try:\n",
        "        modelo_peft.save_pretrained(ruta_adaptador)\n",
        "        if tokenizador is not None:\n",
        "            tokenizador.save_pretrained(ruta_adaptador)\n",
        "        print(f\"Adaptador PEFT y tokenizador guardados en: {ruta_adaptador}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el modelo PEFT o el tokenizador: {e}\")\n",
        "\n",
        "def cargar_modelo_personalizado(ruta_adaptador, nombre_modelo_base=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo base y luego aplica los adaptadores PEFT desde una ruta específica.\n",
        "\n",
        "    Args:\n",
        "        ruta_adaptador (str): Ruta donde se guardaron los adaptadores PEFT (y el tokenizador).\n",
        "        nombre_modelo_base (str, opcional): Nombre o ruta del modelo base original.\n",
        "                                         PEFT >=0.7.0 puede inferirlo si se guardó con el adaptador.\n",
        "                                         Si no, debe proporcionarse.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo_peft, tokenizador) o (None, None) si falla.\n",
        "    \"\"\"\n",
        "    global modelo_global, tokenizador_global, dispositivo_global\n",
        "    if dispositivo_global is None:\n",
        "        dispositivo_global = verificar_dispositivo()\n",
        "\n",
        "    try:\n",
        "        # Cargar tokenizador\n",
        "        tokenizador = AutoTokenizer.from_pretrained(ruta_adaptador)\n",
        "        if tokenizador.pad_token is None:\n",
        "            tokenizador.pad_token = tokenizador.eos_token\n",
        "\n",
        "        # Configuración para cargar el modelo base (puede ser cuantizado)\n",
        "        # Intentar leer la configuración del modelo base desde el adaptador PEFT si existe\n",
        "        peft_config = PeftConfig.from_pretrained(ruta_adaptador)\n",
        "        nombre_modelo_base_resolved = nombre_modelo_base or peft_config.base_model_name_or_path\n",
        "\n",
        "        print(f\"Cargando modelo base '{nombre_modelo_base_resolved}' para aplicar adaptadores PEFT.\")\n",
        "        bnb_config_base = None\n",
        "\n",
        "        modelo_base = AutoModelForCausalLM.from_pretrained(\n",
        "            nombre_modelo_base_resolved,\n",
        "            quantization_config=bnb_config_base, # Aplicar si el base fue cuantizado\n",
        "            torch_dtype=torch.float16 if dispositivo_global.type == 'cuda' and bnb_config_base is None else None,\n",
        "            device_map=\"auto\" if dispositivo_global.type == 'cuda' else None,\n",
        "        )\n",
        "        print(f\"Modelo base '{nombre_modelo_base_resolved}' cargado.\")\n",
        "\n",
        "        # Aplicar adaptadores PEFT\n",
        "        modelo_peft = PeftModel.from_pretrained(modelo_base, ruta_adaptador)\n",
        "        modelo_peft = modelo_peft.merge_and_unload() # Opcional: fusionar para inferencia más rápida si no se va a entrenar más\n",
        "                                                    # Esto crea un nuevo modelo estándar. Si se quiere seguir entrenando, omitir.\n",
        "                                                    # Para inferencia, fusionar es bueno.\n",
        "        print(f\"Adaptadores PEFT de '{ruta_adaptador}' aplicados al modelo base.\")\n",
        "\n",
        "        modelo_peft.eval()\n",
        "        if dispositivo_global.type == 'cuda' and bnb_config_base is None : # Si no está cuantizado y hay GPU\n",
        "             if not hasattr(modelo_peft, 'hf_device_map'): # si no tiene device_map (no multi-gpu/offload)\n",
        "                modelo_peft = modelo_peft.to(dispositivo_global)\n",
        "\n",
        "        modelo_global = modelo_peft\n",
        "        tokenizador_global = tokenizador\n",
        "        return modelo_peft, tokenizador\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo PEFT personalizado: {e}\")\n",
        "        print(\"Asegúrate de que 'ruta_adaptador' sea correcta y que 'nombre_modelo_base' (si es necesario) también lo sea.\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- Interfaz Web con Gradio ---\n",
        "chatbot_instance = None # Variable global para la instancia del chatbot\n",
        "\n",
        "def inicializar_chatbot_global(modelo_id_o_path, instrucciones_sistema, es_personalizado, nombre_modelo_base_peft=None):\n",
        "    \"\"\"Inicializa o reinicializa la instancia global del chatbot.\"\"\"\n",
        "    global chatbot_instance, modelo_global, tokenizador_global\n",
        "    print(\"Limpiando instancias previas del modelo y chatbot...\")\n",
        "    if modelo_global:\n",
        "        del modelo_global\n",
        "        modelo_global = None\n",
        "    if tokenizador_global:\n",
        "        del tokenizador_global\n",
        "        tokenizador_global = None\n",
        "    if chatbot_instance:\n",
        "        del chatbot_instance\n",
        "        chatbot_instance = None\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Inicializando chatbot con: {modelo_id_o_path}\")\n",
        "    if es_personalizado:\n",
        "        chatbot_instance = Chatbot(modelo_id_o_path,\n",
        "                                   instrucciones_sistema=instrucciones_sistema,\n",
        "                                   es_personalizado=True) # El constructor de Chatbot llamará a cargar_modelo_personalizado\n",
        "    else:\n",
        "        chatbot_instance = Chatbot(modelo_id_o_path,\n",
        "                                   instrucciones_sistema=instrucciones_sistema,\n",
        "                                   es_personalizado=False,\n",
        "                                   optimizaciones_carga={\"cuantizacion\": True, \"bits\": 4} if dispositivo_global.type == 'cuda' else None) # Cargar con cuantización si hay GPU\n",
        "    return \"Chatbot inicializado.\"\n",
        "\n",
        "def chatbot_respuesta_gradio(mensaje_usuario, historial_chat):\n",
        "    \"\"\"\n",
        "    Función de callback para Gradio.\n",
        "    historial_chat es una lista de tuplas [(user_msg1, bot_msg1), (user_msg2, bot_msg2), ...]\n",
        "    \"\"\"\n",
        "    global chatbot_instance\n",
        "    if chatbot_instance is None:\n",
        "        print(\"Advertencia: Chatbot no inicializado. Intentando inicialización por defecto.\")\n",
        "        return \"Error: El chatbot no está inicializado. Por favor, carga un modelo.\", historial_chat\n",
        "\n",
        "    print(f\"Usuario (Gradio): {mensaje_usuario}\")\n",
        "    respuesta = chatbot_instance.responder(mensaje_usuario)\n",
        "    print(f\"Chatbot (Gradio): {respuesta}\")\n",
        "    return respuesta # Gradio ChatInterface espera solo la respuesta del bot\n",
        "\n",
        "def crear_interfaz_web():\n",
        "    \"\"\"\n",
        "    Crea una interfaz web para el chatbot usando Gradio.\n",
        "    \"\"\"\n",
        "    global chatbot_instance # Necesario para la función de respuesta\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as interfaz:\n",
        "        gr.Markdown(\"# Chatbot Personalizado con PEFT y Gradio\")\n",
        "        gr.Markdown(\"Carga un modelo base o un modelo afinado con PEFT, y luego chatea con él.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                modelo_id_input = gr.Textbox(label=\"ID/Ruta Modelo Base o Ruta Adaptador PEFT\", value=\"gpt2-medium\") #gpt2-medium para prueba más robusta\n",
        "                instrucciones_input = gr.Textbox(label=\"Instrucciones del Sistema\", value=\"Eres un asistente IA muy útil y creativo.\")\n",
        "                es_personalizado_checkbox = gr.Checkbox(label=\"¿Cargar desde ruta de adaptador PEFT?\", value=False)\n",
        "                # nombre_modelo_base_peft_input = gr.Textbox(label=\"Nombre/Ruta del Modelo Base (si carga PEFT y no está en config)\", placeholder=\"Ej: gpt2\")\n",
        "                cargar_button = gr.Button(\"🚀 Cargar y Configurar Chatbot\")\n",
        "                status_output = gr.Label(label=\"Estado del Chatbot\")\n",
        "\n",
        "        # La función de carga ahora toma todos los parámetros necesarios\n",
        "        cargar_button.click(\n",
        "            fn=lambda id_path, instr, es_pers: inicializar_chatbot_global(id_path, instr, es_pers),\n",
        "            inputs=[modelo_id_input, instrucciones_input, es_personalizado_checkbox], # , nombre_modelo_base_peft_input\n",
        "            outputs=status_output\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"## Conversación\")\n",
        "        # Usar gr.ChatInterface dentro de Blocks para más control\n",
        "        # Necesitamos pasar una función que solo toma (mensaje, historial)\n",
        "        def chat_fn_wrapper(message, history):\n",
        "\n",
        "            # Simplificado: chatbot_instance mantiene su propio historial. Gradio solo muestra.\n",
        "            if chatbot_instance is None:\n",
        "                return \"Error: El chatbot no está inicializado. Por favor, carga un modelo primero.\"\n",
        "\n",
        "            print(f\"Historial Gradio (antes): {history}\")\n",
        "            print(f\"Mensaje Usuario (Gradio): {message}\")\n",
        "            respuesta = chatbot_instance.responder(message) # El chatbot interno usa su propio contexto\n",
        "            print(f\"Respuesta Chatbot (Gradio): {respuesta}\")\n",
        "            return respuesta\n",
        "\n",
        "        gr.ChatInterface(\n",
        "            fn=chat_fn_wrapper,\n",
        "            examples=[[\"¿Cómo estás hoy?\"], [\"Explícame qué es un transformador en IA.\"]],\n",
        "        )\n",
        "\n",
        "    return interfaz\n",
        "\n",
        "\n",
        "# --- Flujo Principal ---\n",
        "\n",
        "def main_entrenamiento_ejemplo(nombre_modelo_base=\"gpt2\", ruta_guardado_peft=\"./peft_adaptadores\"):\n",
        "    \"\"\"\n",
        "    Ejemplo de cómo configurar PEFT y simular un paso de \"entrenamiento\".\n",
        "    En un caso real, aquí iría el bucle de entrenamiento con tus datos.\n",
        "    \"\"\"\n",
        "    print(\"--- Iniciando Flujo de Entrenamiento de Ejemplo con PEFT ---\")\n",
        "    modelo_base, tokenizador_base = cargar_modelo_base(nombre_modelo_base, optimizaciones={\"cuantizacion\": True, \"bits\": 4} if dispositivo_global.type == 'cuda' else None)\n",
        "\n",
        "    if modelo_base is None or tokenizador_base is None:\n",
        "        print(\"No se pudo cargar el modelo base. Abortando entrenamiento de ejemplo.\")\n",
        "        return\n",
        "\n",
        "    # Especificar target_modules para gpt2\n",
        "    # Para gpt2, 'c_attn' es la capa principal de atención donde se aplican Q, K, V.\n",
        "    # Si se usa otro modelo, estos módulos deben cambiar.\n",
        "    target_modules_gpt2 = [\"c_attn\"]\n",
        "\n",
        "    print(f\"Configurando PEFT para el modelo base: {nombre_modelo_base}\")\n",
        "    modelo_peft = configurar_peft(\n",
        "        modelo_base,\n",
        "        r=16,  # Rango LoRA, mayor puede dar más capacidad pero más parámetros entrenables\n",
        "        lora_alpha=32, # Usualmente 2*r\n",
        "        lora_dropout=0.1,\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        target_modules=target_modules_gpt2 if \"gpt2\" in nombre_modelo_base else None # Ajustar si es otro modelo\n",
        "    )\n",
        "\n",
        "    if modelo_peft == modelo_base: # Si configurar_peft falló y devolvió el original\n",
        "        print(\"La configuración de PEFT falló. Abortando.\")\n",
        "        return\n",
        "\n",
        "    print(\"Simulación de entrenamiento completada (paso omitido).\")\n",
        "\n",
        "\n",
        "    # Guardar los adaptadores PEFT\n",
        "    guardar_modelo_peft(modelo_peft, tokenizador_base, ruta_guardado_peft, nombre_adaptador=f\"{nombre_modelo_base.replace('/','_')}_lora\")\n",
        "    print(\"--- Fin del Flujo de Entrenamiento de Ejemplo con PEFT ---\")\n",
        "\n",
        "    # Limpiar memoria\n",
        "    del modelo_base, modelo_peft, tokenizador_base\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def main_despliegue(modelo_a_cargar=\"gpt2-medium\", # Puede ser un ID de Hugging Face o una ruta a adaptadores PEFT\n",
        "                    instrucciones=\"Eres un asistente virtual que ayuda a los usuarios con sus preguntas.\",\n",
        "                    es_modelo_peft=False, # True si modelo_a_cargar es una ruta a adaptadores PEFT\n",
        "                    nombre_modelo_base_para_peft=\"gpt2-medium\"): # Necesario si es_modelo_peft es True y no está en config\n",
        "    \"\"\"\n",
        "    Función principal para configurar y lanzar la interfaz web del chatbot.\n",
        "    \"\"\"\n",
        "    global chatbot_instance, modelo_global, tokenizador_global # Para que Gradio pueda accederlos\n",
        "    print(\"--- Iniciando Despliegue del Chatbot ---\")\n",
        "\n",
        "    # Inicializar el dispositivo global\n",
        "    verificar_dispositivo()\n",
        "\n",
        "    # La inicialización del chatbot ahora se hace a través de la UI de Gradio\n",
        "    # o se podría hacer una carga inicial aquí si se desea.\n",
        "\n",
        "    # Si queremos cargar un modelo por defecto al inicio:\n",
        "    print(f\"Cargando modelo inicial por defecto: {modelo_a_cargar}\")\n",
        "\n",
        "    # Crear y lanzar la interfaz web\n",
        "    interfaz = crear_interfaz_web()\n",
        "    print(\"Lanzando interfaz de Gradio...\")\n",
        "    interfaz.launch()\n",
        "\n",
        "    print(\"--- Interfaz de Gradio cerrada ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_despliegue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "49ACfnoWqgb6",
        "outputId": "257a1e76-3f80-4b56-bf8c-7a1e9b45eb38"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La caché de modelos se guardará en: ./cache_modelos\n",
            "--- Iniciando Despliegue del Chatbot ---\n",
            "No se encontró GPU disponible, utilizando CPU.\n",
            "Cargando modelo inicial por defecto: gpt2-medium\n",
            "Lanzando interfaz de Gradio...\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0e8ce4b38c98bf25eb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e8ce4b38c98bf25eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Interfaz de Gradio cerrada ---\n"
          ]
        }
      ]
    }
  ]
}